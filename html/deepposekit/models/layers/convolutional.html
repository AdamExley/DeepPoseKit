<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.7.0" />
<title>deepposekit.models.layers.convolutional API documentation</title>
<meta name="description" content="" />
<link href='https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.0/normalize.min.css' rel='stylesheet'>
<link href='https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/8.0.0/sanitize.min.css' rel='stylesheet'>
<link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css" rel="stylesheet">
<style>.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{font-weight:bold}#index h4 + ul{margin-bottom:.6em}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>deepposekit.models.layers.convolutional</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python"># -*- coding: utf-8 -*-
# Copyright 2018-2019 Jacob M. Graving &lt;jgraving@gmail.com&gt;
#
# Licensed under the Apache License, Version 2.0 (the &#34;License&#34;);
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at

#    http://www.apache.org/licenses/LICENSE-2.0

# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an &#34;AS IS&#34; BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from tensorflow.python.keras.engine import Layer
from tensorflow.python.keras.engine import InputSpec

from tensorflow.keras.layers import UpSampling2D

from deepposekit.models.backend.backend import (
    resize_images,
    find_maxima,
    depth_to_space,
    space_to_depth,
)

from tensorflow.python.keras.utils.conv_utils import (
    normalize_data_format,
    normalize_tuple,
)

__all__ = [&#34;UpSampling2D&#34;, &#34;Maxima2D&#34;, &#34;SubPixelUpscaling&#34;, &#34;SubPixelDownscaling&#34;]


class Maxima2D(Layer):
    &#34;&#34;&#34;Maxima layer for 2D inputs.
    Finds the maxima and 2D indices
    for the channels in the input.
    The output is ordered as [row, col, maximum].
    # Arguments
        index: Integer,
            The index to slice the channels to.
            Default is None, which does not slice the channels.
        data_format: A string,
            one of `channels_last` (default) or `channels_first`.
            The ordering of the dimensions in the inputs.
            `channels_last` corresponds to inputs with shape
            `(batch, height, width, channels)` while `channels_first`
            corresponds to inputs with shape
            `(batch, channels, height, width)`.
            It defaults to the `image_data_format` value found in your
            Keras config file at `~/.keras/keras.json`.
            If you never set it, then it will be &#34;channels_last&#34;.
    # Input shape
        4D tensor with shape:
        - If `data_format` is `&#34;channels_last&#34;`:
            `(batch, rows, cols, channels)`
        - If `data_format` is `&#34;channels_first&#34;`:
            `(batch, channels, rows, cols)`
    # Output shape
        3D tensor with shape:
        - If `data_format` is `&#34;channels_last&#34;`:
            `(batch, 3, index)`
        - If `data_format` is `&#34;channels_first&#34;`:
            `(batch, index, 3)`
    &#34;&#34;&#34;

    def __init__(
        self,
        index=None,
        coordinate_scale=1.0,
        confidence_scale=1.0,
        data_format=None,
        **kwargs
    ):
        super(Maxima2D, self).__init__(**kwargs)
        self.data_format = normalize_data_format(data_format)
        self.input_spec = InputSpec(ndim=4)
        self.index = index
        self.coordinate_scale = coordinate_scale
        self.confidence_scale = confidence_scale

    def compute_output_shape(self, input_shape):
        if self.data_format == &#34;channels_first&#34;:
            n_channels = self.index if self.index is not None else input_shape[1]

        elif self.data_format == &#34;channels_last&#34;:
            n_channels = self.index if self.index is not None else input_shape[3]
        return (input_shape[0], n_channels, 3)

    def call(self, inputs):
        if self.data_format == &#34;channels_first&#34;:
            inputs = inputs[:, : self.index]
        elif self.data_format == &#34;channels_last&#34;:
            inputs = inputs[..., : self.index]
        outputs = find_maxima(
            inputs, self.coordinate_scale, self.confidence_scale, self.data_format
        )
        return outputs

    def get_config(self):
        config = {
            &#34;data_format&#34;: self.data_format,
            &#34;index&#34;: self.index,
            &#34;coordinate_scale&#34;: self.coordinate_scale,
            &#34;confidence_scale&#34;: self.confidence_scale,
        }
        base_config = super(Maxima2D, self).get_config()
        return dict(list(base_config.items()) + list(config.items()))


class SubPixelUpscaling(Layer):
    &#34;&#34;&#34; Sub-pixel convolutional upscaling layer based on the paper &#34;Real-Time Single Image
    and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network&#34;
    (https://arxiv.org/abs/1609.05158).
    This layer requires a Convolution2D prior to it, having output filters computed according to
    the formula :
        filters = k * (scale_factor * scale_factor)
        where k = a user defined number of filters (generally larger than 32)
              scale_factor = the upscaling factor (generally 2)
    This layer performs the depth to space operation on the convolution filters, and returns a
    tensor with the size as defined below.
    # Example :
    ```python
        # A standard subpixel upscaling block
        x = Convolution2D(256, 3, 3, padding=&#39;same&#39;, activation=&#39;relu&#39;)(...)
        u = SubPixelUpscaling(scale_factor=2)(x)
        [Optional]
        x = Convolution2D(256, 3, 3, padding=&#39;same&#39;, activation=&#39;relu&#39;)(u)
    ```
        In practice, it is useful to have a second convolution layer after the
        SubPixelUpscaling layer to speed up the learning process.
        However, if you are stacking multiple SubPixelUpscaling blocks, it may increase
        the number of parameters greatly, so the Convolution layer after SubPixelUpscaling
        layer can be removed.
    # Arguments
        scale_factor: Upscaling factor.
        data_format: Can be None, &#39;channels_first&#39; or &#39;channels_last&#39;.
    # Input shape
        4D tensor with shape:
        `(samples, k * (scale_factor * scale_factor) channels, rows, cols)` if data_format=&#39;channels_first&#39;
        or 4D tensor with shape:
        `(samples, rows, cols, k * (scale_factor * scale_factor) channels)` if data_format=&#39;channels_last&#39;.
    # Output shape
        4D tensor with shape:
        `(samples, k channels, rows * scale_factor, cols * scale_factor))` if data_format=&#39;channels_first&#39;
        or 4D tensor with shape:
        `(samples, rows * scale_factor, cols * scale_factor, k channels)` if data_format=&#39;channels_last&#39;.
    &#34;&#34;&#34;

    def __init__(self, scale_factor=2, data_format=None, **kwargs):
        super(SubPixelUpscaling, self).__init__(**kwargs)

        self.scale_factor = scale_factor
        self.data_format = normalize_data_format(data_format)

    def build(self, input_shape):
        pass

    def call(self, x, mask=None):
        y = depth_to_space(x, self.scale_factor, self.data_format)
        return y

    def compute_output_shape(self, input_shape):
        if self.data_format == &#34;channels_first&#34;:
            b, k, r, c = input_shape
            return (
                b,
                k // (self.scale_factor ** 2),
                r * self.scale_factor,
                c * self.scale_factor,
            )
        else:
            b, r, c, k = input_shape
            return (
                b,
                r * self.scale_factor,
                c * self.scale_factor,
                k // (self.scale_factor ** 2),
            )

    def get_config(self):
        config = {&#34;scale_factor&#34;: self.scale_factor, &#34;data_format&#34;: self.data_format}
        base_config = super(SubPixelUpscaling, self).get_config()
        return dict(list(base_config.items()) + list(config.items()))


class SubPixelDownscaling(Layer):
    &#34;&#34;&#34; Sub-pixel convolutional downscaling layer based on the paper &#34;Real-Time Single Image
    and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network&#34;
    (https://arxiv.org/abs/1609.05158).
    This layer requires a Convolution2D prior to it, having output filters computed according to
    the formula :
        filters = k * (scale_factor * scale_factor)
        where k = a user defined number of filters (generally larger than 32)
              scale_factor = the upscaling factor (generally 2)
    This layer performs the depth to space operation on the convolution filters, and returns a
    tensor with the size as defined below.
    # Example :
    ```python
        # A standard subpixel upscaling block
        x = Convolution2D(256, 3, 3, padding=&#39;same&#39;, activation=&#39;relu&#39;)(...)
        u = SubPixelDownscaling(scale_factor=2)(x)
        [Optional]
        x = Convolution2D(256, 3, 3, padding=&#39;same&#39;, activation=&#39;relu&#39;)(u)
    ```
        In practice, it is useful to have a second convolution layer after the
        SubPixelUpscaling layer to speed up the learning process.
        However, if you are stacking multiple SubPixelUpscaling blocks, it may increase
        the number of parameters greatly, so the Convolution layer after SubPixelUpscaling
        layer can be removed.
    # Arguments
        scale_factor: Upscaling factor.
        data_format: Can be None, &#39;channels_first&#39; or &#39;channels_last&#39;.
    # Input shape
        4D tensor with shape:
        `(samples, k * (scale_factor * scale_factor) channels, rows, cols)` if data_format=&#39;channels_first&#39;
        or 4D tensor with shape:
        `(samples, rows, cols, k * (scale_factor * scale_factor) channels)` if data_format=&#39;channels_last&#39;.
    # Output shape
        4D tensor with shape:
        `(samples, k channels, rows * scale_factor, cols * scale_factor))` if data_format=&#39;channels_first&#39;
        or 4D tensor with shape:
        `(samples, rows * scale_factor, cols * scale_factor, k channels)` if data_format=&#39;channels_last&#39;.
    &#34;&#34;&#34;

    def __init__(self, scale_factor=2, data_format=None, **kwargs):
        super(SubPixelDownscaling, self).__init__(**kwargs)

        self.scale_factor = scale_factor
        self.data_format = normalize_data_format(data_format)

    def build(self, input_shape):
        pass

    def call(self, x, mask=None):
        y = space_to_depth(x, self.scale_factor, self.data_format)
        return y

    def compute_output_shape(self, input_shape):
        if self.data_format == &#34;channels_first&#34;:
            b, k, r, c = input_shape
            return (
                b,
                k // (self.scale_factor ** 2),
                r // self.scale_factor,
                c // self.scale_factor,
            )
        else:
            b, r, c, k = input_shape
            return (
                b,
                r // self.scale_factor,
                c // self.scale_factor,
                k * (self.scale_factor ** 2),
            )

    def get_config(self):
        config = {&#34;scale_factor&#34;: self.scale_factor, &#34;data_format&#34;: self.data_format}
        base_config = super(SubPixelDownscaling, self).get_config()
        return dict(list(base_config.items()) + list(config.items()))</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="deepposekit.models.layers.convolutional.Maxima2D"><code class="flex name class">
<span>class <span class="ident">Maxima2D</span></span>
<span>(</span><span>index=None, coordinate_scale=1.0, confidence_scale=1.0, data_format=None, **kwargs)</span>
</code></dt>
<dd>
<section class="desc"><p>Maxima layer for 2D inputs.
Finds the maxima and 2D indices
for the channels in the input.
The output is ordered as [row, col, maximum].</p>
<h1 id="arguments">Arguments</h1>
<pre><code>index: Integer,
    The index to slice the channels to.
    Default is None, which does not slice the channels.
data_format: A string,
    one of `channels_last` (default) or `channels_first`.
    The ordering of the dimensions in the inputs.
    `channels_last` corresponds to inputs with shape
    `(batch, height, width, channels)` while `channels_first`
    corresponds to inputs with shape
    `(batch, channels, height, width)`.
    It defaults to the `image_data_format` value found in your
    Keras config file at `~/.keras/keras.json`.
    If you never set it, then it will be "channels_last".
</code></pre>
<h1 id="input-shape">Input shape</h1>
<pre><code>4D tensor with shape:
- If `data_format` is `"channels_last"`:
    `(batch, rows, cols, channels)`
- If `data_format` is `"channels_first"`:
    `(batch, channels, rows, cols)`
</code></pre>
<h1 id="output-shape">Output shape</h1>
<pre><code>3D tensor with shape:
- If `data_format` is `"channels_last"`:
    `(batch, 3, index)`
- If `data_format` is `"channels_first"`:
    `(batch, index, 3)`
</code></pre></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Maxima2D(Layer):
    &#34;&#34;&#34;Maxima layer for 2D inputs.
    Finds the maxima and 2D indices
    for the channels in the input.
    The output is ordered as [row, col, maximum].
    # Arguments
        index: Integer,
            The index to slice the channels to.
            Default is None, which does not slice the channels.
        data_format: A string,
            one of `channels_last` (default) or `channels_first`.
            The ordering of the dimensions in the inputs.
            `channels_last` corresponds to inputs with shape
            `(batch, height, width, channels)` while `channels_first`
            corresponds to inputs with shape
            `(batch, channels, height, width)`.
            It defaults to the `image_data_format` value found in your
            Keras config file at `~/.keras/keras.json`.
            If you never set it, then it will be &#34;channels_last&#34;.
    # Input shape
        4D tensor with shape:
        - If `data_format` is `&#34;channels_last&#34;`:
            `(batch, rows, cols, channels)`
        - If `data_format` is `&#34;channels_first&#34;`:
            `(batch, channels, rows, cols)`
    # Output shape
        3D tensor with shape:
        - If `data_format` is `&#34;channels_last&#34;`:
            `(batch, 3, index)`
        - If `data_format` is `&#34;channels_first&#34;`:
            `(batch, index, 3)`
    &#34;&#34;&#34;

    def __init__(
        self,
        index=None,
        coordinate_scale=1.0,
        confidence_scale=1.0,
        data_format=None,
        **kwargs
    ):
        super(Maxima2D, self).__init__(**kwargs)
        self.data_format = normalize_data_format(data_format)
        self.input_spec = InputSpec(ndim=4)
        self.index = index
        self.coordinate_scale = coordinate_scale
        self.confidence_scale = confidence_scale

    def compute_output_shape(self, input_shape):
        if self.data_format == &#34;channels_first&#34;:
            n_channels = self.index if self.index is not None else input_shape[1]

        elif self.data_format == &#34;channels_last&#34;:
            n_channels = self.index if self.index is not None else input_shape[3]
        return (input_shape[0], n_channels, 3)

    def call(self, inputs):
        if self.data_format == &#34;channels_first&#34;:
            inputs = inputs[:, : self.index]
        elif self.data_format == &#34;channels_last&#34;:
            inputs = inputs[..., : self.index]
        outputs = find_maxima(
            inputs, self.coordinate_scale, self.confidence_scale, self.data_format
        )
        return outputs

    def get_config(self):
        config = {
            &#34;data_format&#34;: self.data_format,
            &#34;index&#34;: self.index,
            &#34;coordinate_scale&#34;: self.coordinate_scale,
            &#34;confidence_scale&#34;: self.confidence_scale,
        }
        base_config = super(Maxima2D, self).get_config()
        return dict(list(base_config.items()) + list(config.items()))</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>tensorflow.python.keras.engine.base_layer.Layer</li>
<li>tensorflow.python.module.module.Module</li>
<li>tensorflow.python.training.tracking.tracking.AutoTrackable</li>
<li>tensorflow.python.training.tracking.base.Trackable</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="deepposekit.models.layers.convolutional.Maxima2D.call"><code class="name flex">
<span>def <span class="ident">call</span></span>(<span>self, inputs)</span>
</code></dt>
<dd>
<section class="desc"><p>This is where the layer's logic lives.</p>
<h2 id="arguments">Arguments</h2>
<dl>
<dt><strong><code>inputs</code></strong></dt>
<dd>Input tensor, or list/tuple of input tensors.</dd>
<dt><strong><code>**kwargs</code></strong></dt>
<dd>Additional keyword arguments.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>A tensor or list/tuple of tensors.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def call(self, inputs):
    if self.data_format == &#34;channels_first&#34;:
        inputs = inputs[:, : self.index]
    elif self.data_format == &#34;channels_last&#34;:
        inputs = inputs[..., : self.index]
    outputs = find_maxima(
        inputs, self.coordinate_scale, self.confidence_scale, self.data_format
    )
    return outputs</code></pre>
</details>
</dd>
<dt id="deepposekit.models.layers.convolutional.Maxima2D.compute_output_shape"><code class="name flex">
<span>def <span class="ident">compute_output_shape</span></span>(<span>self, input_shape)</span>
</code></dt>
<dd>
<section class="desc"><p>Computes the output shape of the layer.</p>
<p>Assumes that the layer will be built
to match that input shape provided.</p>
<h2 id="arguments">Arguments</h2>
<dl>
<dt><strong><code>input_shape</code></strong></dt>
<dd>Shape tuple (tuple of integers)
or list of shape tuples (one per output tensor of the layer).
Shape tuples can include None for free dimensions,
instead of an integer.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>An input shape tuple.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def compute_output_shape(self, input_shape):
    if self.data_format == &#34;channels_first&#34;:
        n_channels = self.index if self.index is not None else input_shape[1]

    elif self.data_format == &#34;channels_last&#34;:
        n_channels = self.index if self.index is not None else input_shape[3]
    return (input_shape[0], n_channels, 3)</code></pre>
</details>
</dd>
<dt id="deepposekit.models.layers.convolutional.Maxima2D.get_config"><code class="name flex">
<span>def <span class="ident">get_config</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"><p>Returns the config of the layer.</p>
<p>A layer config is a Python dictionary (serializable)
containing the configuration of a layer.
The same layer can be reinstantiated later
(without its trained weights) from this configuration.</p>
<p>The config of a layer does not include connectivity
information, nor the layer class name. These are handled
by <code>Network</code> (one layer of abstraction above).</p>
<h2 id="returns">Returns</h2>
<p>Python dictionary.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_config(self):
    config = {
        &#34;data_format&#34;: self.data_format,
        &#34;index&#34;: self.index,
        &#34;coordinate_scale&#34;: self.coordinate_scale,
        &#34;confidence_scale&#34;: self.confidence_scale,
    }
    base_config = super(Maxima2D, self).get_config()
    return dict(list(base_config.items()) + list(config.items()))</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="deepposekit.models.layers.convolutional.SubPixelDownscaling"><code class="flex name class">
<span>class <span class="ident">SubPixelDownscaling</span></span>
<span>(</span><span>scale_factor=2, data_format=None, **kwargs)</span>
</code></dt>
<dd>
<section class="desc"><p>Sub-pixel convolutional downscaling layer based on the paper "Real-Time Single Image
and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network"
(<a href="https://arxiv.org/abs/1609.05158">https://arxiv.org/abs/1609.05158</a>).
This layer requires a Convolution2D prior to it, having output filters computed according to
the formula :
filters = k * (scale_factor * scale_factor)
where k = a user defined number of filters (generally larger than 32)
scale_factor = the upscaling factor (generally 2)
This layer performs the depth to space operation on the convolution filters, and returns a
tensor with the size as defined below.</p>
<h1 id="example">Example :</h1>
<pre><code class="python">    # A standard subpixel upscaling block
    x = Convolution2D(256, 3, 3, padding='same', activation='relu')(...)
    u = SubPixelDownscaling(scale_factor=2)(x)
    [Optional]
    x = Convolution2D(256, 3, 3, padding='same', activation='relu')(u)
</code></pre>
<pre><code>In practice, it is useful to have a second convolution layer after the
SubPixelUpscaling layer to speed up the learning process.
However, if you are stacking multiple SubPixelUpscaling blocks, it may increase
the number of parameters greatly, so the Convolution layer after SubPixelUpscaling
layer can be removed.
</code></pre>
<h1 id="arguments">Arguments</h1>
<pre><code>scale_factor: Upscaling factor.
data_format: Can be None, 'channels_first' or 'channels_last'.
</code></pre>
<h1 id="input-shape">Input shape</h1>
<pre><code>4D tensor with shape:
`(samples, k * (scale_factor * scale_factor) channels, rows, cols)` if data_format='channels_first'
or 4D tensor with shape:
`(samples, rows, cols, k * (scale_factor * scale_factor) channels)` if data_format='channels_last'.
</code></pre>
<h1 id="output-shape">Output shape</h1>
<pre><code>4D tensor with shape:
`(samples, k channels, rows * scale_factor, cols * scale_factor))` if data_format='channels_first'
or 4D tensor with shape:
`(samples, rows * scale_factor, cols * scale_factor, k channels)` if data_format='channels_last'.
</code></pre></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class SubPixelDownscaling(Layer):
    &#34;&#34;&#34; Sub-pixel convolutional downscaling layer based on the paper &#34;Real-Time Single Image
    and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network&#34;
    (https://arxiv.org/abs/1609.05158).
    This layer requires a Convolution2D prior to it, having output filters computed according to
    the formula :
        filters = k * (scale_factor * scale_factor)
        where k = a user defined number of filters (generally larger than 32)
              scale_factor = the upscaling factor (generally 2)
    This layer performs the depth to space operation on the convolution filters, and returns a
    tensor with the size as defined below.
    # Example :
    ```python
        # A standard subpixel upscaling block
        x = Convolution2D(256, 3, 3, padding=&#39;same&#39;, activation=&#39;relu&#39;)(...)
        u = SubPixelDownscaling(scale_factor=2)(x)
        [Optional]
        x = Convolution2D(256, 3, 3, padding=&#39;same&#39;, activation=&#39;relu&#39;)(u)
    ```
        In practice, it is useful to have a second convolution layer after the
        SubPixelUpscaling layer to speed up the learning process.
        However, if you are stacking multiple SubPixelUpscaling blocks, it may increase
        the number of parameters greatly, so the Convolution layer after SubPixelUpscaling
        layer can be removed.
    # Arguments
        scale_factor: Upscaling factor.
        data_format: Can be None, &#39;channels_first&#39; or &#39;channels_last&#39;.
    # Input shape
        4D tensor with shape:
        `(samples, k * (scale_factor * scale_factor) channels, rows, cols)` if data_format=&#39;channels_first&#39;
        or 4D tensor with shape:
        `(samples, rows, cols, k * (scale_factor * scale_factor) channels)` if data_format=&#39;channels_last&#39;.
    # Output shape
        4D tensor with shape:
        `(samples, k channels, rows * scale_factor, cols * scale_factor))` if data_format=&#39;channels_first&#39;
        or 4D tensor with shape:
        `(samples, rows * scale_factor, cols * scale_factor, k channels)` if data_format=&#39;channels_last&#39;.
    &#34;&#34;&#34;

    def __init__(self, scale_factor=2, data_format=None, **kwargs):
        super(SubPixelDownscaling, self).__init__(**kwargs)

        self.scale_factor = scale_factor
        self.data_format = normalize_data_format(data_format)

    def build(self, input_shape):
        pass

    def call(self, x, mask=None):
        y = space_to_depth(x, self.scale_factor, self.data_format)
        return y

    def compute_output_shape(self, input_shape):
        if self.data_format == &#34;channels_first&#34;:
            b, k, r, c = input_shape
            return (
                b,
                k // (self.scale_factor ** 2),
                r // self.scale_factor,
                c // self.scale_factor,
            )
        else:
            b, r, c, k = input_shape
            return (
                b,
                r // self.scale_factor,
                c // self.scale_factor,
                k * (self.scale_factor ** 2),
            )

    def get_config(self):
        config = {&#34;scale_factor&#34;: self.scale_factor, &#34;data_format&#34;: self.data_format}
        base_config = super(SubPixelDownscaling, self).get_config()
        return dict(list(base_config.items()) + list(config.items()))</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>tensorflow.python.keras.engine.base_layer.Layer</li>
<li>tensorflow.python.module.module.Module</li>
<li>tensorflow.python.training.tracking.tracking.AutoTrackable</li>
<li>tensorflow.python.training.tracking.base.Trackable</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="deepposekit.models.layers.convolutional.SubPixelDownscaling.build"><code class="name flex">
<span>def <span class="ident">build</span></span>(<span>self, input_shape)</span>
</code></dt>
<dd>
<section class="desc"><p>Creates the variables of the layer (optional, for subclass implementers).</p>
<p>This is a method that implementers of subclasses of <code>Layer</code> or <code>Model</code>
can override if they need a state-creation step in-between
layer instantiation and layer call.</p>
<p>This is typically used to create the weights of <code>Layer</code> subclasses.</p>
<h2 id="arguments">Arguments</h2>
<dl>
<dt><strong><code>input_shape</code></strong></dt>
<dd>Instance of <code>TensorShape</code>, or list of instances of
<code>TensorShape</code> if the layer expects a list of inputs
(one instance per input).</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def build(self, input_shape):
    pass</code></pre>
</details>
</dd>
<dt id="deepposekit.models.layers.convolutional.SubPixelDownscaling.call"><code class="name flex">
<span>def <span class="ident">call</span></span>(<span>self, x, mask=None)</span>
</code></dt>
<dd>
<section class="desc"><p>This is where the layer's logic lives.</p>
<h2 id="arguments">Arguments</h2>
<dl>
<dt><strong><code>inputs</code></strong></dt>
<dd>Input tensor, or list/tuple of input tensors.</dd>
<dt><strong><code>**kwargs</code></strong></dt>
<dd>Additional keyword arguments.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>A tensor or list/tuple of tensors.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def call(self, x, mask=None):
    y = space_to_depth(x, self.scale_factor, self.data_format)
    return y</code></pre>
</details>
</dd>
<dt id="deepposekit.models.layers.convolutional.SubPixelDownscaling.compute_output_shape"><code class="name flex">
<span>def <span class="ident">compute_output_shape</span></span>(<span>self, input_shape)</span>
</code></dt>
<dd>
<section class="desc"><p>Computes the output shape of the layer.</p>
<p>Assumes that the layer will be built
to match that input shape provided.</p>
<h2 id="arguments">Arguments</h2>
<dl>
<dt><strong><code>input_shape</code></strong></dt>
<dd>Shape tuple (tuple of integers)
or list of shape tuples (one per output tensor of the layer).
Shape tuples can include None for free dimensions,
instead of an integer.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>An input shape tuple.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def compute_output_shape(self, input_shape):
    if self.data_format == &#34;channels_first&#34;:
        b, k, r, c = input_shape
        return (
            b,
            k // (self.scale_factor ** 2),
            r // self.scale_factor,
            c // self.scale_factor,
        )
    else:
        b, r, c, k = input_shape
        return (
            b,
            r // self.scale_factor,
            c // self.scale_factor,
            k * (self.scale_factor ** 2),
        )</code></pre>
</details>
</dd>
<dt id="deepposekit.models.layers.convolutional.SubPixelDownscaling.get_config"><code class="name flex">
<span>def <span class="ident">get_config</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"><p>Returns the config of the layer.</p>
<p>A layer config is a Python dictionary (serializable)
containing the configuration of a layer.
The same layer can be reinstantiated later
(without its trained weights) from this configuration.</p>
<p>The config of a layer does not include connectivity
information, nor the layer class name. These are handled
by <code>Network</code> (one layer of abstraction above).</p>
<h2 id="returns">Returns</h2>
<p>Python dictionary.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_config(self):
    config = {&#34;scale_factor&#34;: self.scale_factor, &#34;data_format&#34;: self.data_format}
    base_config = super(SubPixelDownscaling, self).get_config()
    return dict(list(base_config.items()) + list(config.items()))</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="deepposekit.models.layers.convolutional.SubPixelUpscaling"><code class="flex name class">
<span>class <span class="ident">SubPixelUpscaling</span></span>
<span>(</span><span>scale_factor=2, data_format=None, **kwargs)</span>
</code></dt>
<dd>
<section class="desc"><p>Sub-pixel convolutional upscaling layer based on the paper "Real-Time Single Image
and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network"
(<a href="https://arxiv.org/abs/1609.05158">https://arxiv.org/abs/1609.05158</a>).
This layer requires a Convolution2D prior to it, having output filters computed according to
the formula :
filters = k * (scale_factor * scale_factor)
where k = a user defined number of filters (generally larger than 32)
scale_factor = the upscaling factor (generally 2)
This layer performs the depth to space operation on the convolution filters, and returns a
tensor with the size as defined below.</p>
<h1 id="example">Example :</h1>
<pre><code class="python">    # A standard subpixel upscaling block
    x = Convolution2D(256, 3, 3, padding='same', activation='relu')(...)
    u = SubPixelUpscaling(scale_factor=2)(x)
    [Optional]
    x = Convolution2D(256, 3, 3, padding='same', activation='relu')(u)
</code></pre>
<pre><code>In practice, it is useful to have a second convolution layer after the
SubPixelUpscaling layer to speed up the learning process.
However, if you are stacking multiple SubPixelUpscaling blocks, it may increase
the number of parameters greatly, so the Convolution layer after SubPixelUpscaling
layer can be removed.
</code></pre>
<h1 id="arguments">Arguments</h1>
<pre><code>scale_factor: Upscaling factor.
data_format: Can be None, 'channels_first' or 'channels_last'.
</code></pre>
<h1 id="input-shape">Input shape</h1>
<pre><code>4D tensor with shape:
`(samples, k * (scale_factor * scale_factor) channels, rows, cols)` if data_format='channels_first'
or 4D tensor with shape:
`(samples, rows, cols, k * (scale_factor * scale_factor) channels)` if data_format='channels_last'.
</code></pre>
<h1 id="output-shape">Output shape</h1>
<pre><code>4D tensor with shape:
`(samples, k channels, rows * scale_factor, cols * scale_factor))` if data_format='channels_first'
or 4D tensor with shape:
`(samples, rows * scale_factor, cols * scale_factor, k channels)` if data_format='channels_last'.
</code></pre></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class SubPixelUpscaling(Layer):
    &#34;&#34;&#34; Sub-pixel convolutional upscaling layer based on the paper &#34;Real-Time Single Image
    and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network&#34;
    (https://arxiv.org/abs/1609.05158).
    This layer requires a Convolution2D prior to it, having output filters computed according to
    the formula :
        filters = k * (scale_factor * scale_factor)
        where k = a user defined number of filters (generally larger than 32)
              scale_factor = the upscaling factor (generally 2)
    This layer performs the depth to space operation on the convolution filters, and returns a
    tensor with the size as defined below.
    # Example :
    ```python
        # A standard subpixel upscaling block
        x = Convolution2D(256, 3, 3, padding=&#39;same&#39;, activation=&#39;relu&#39;)(...)
        u = SubPixelUpscaling(scale_factor=2)(x)
        [Optional]
        x = Convolution2D(256, 3, 3, padding=&#39;same&#39;, activation=&#39;relu&#39;)(u)
    ```
        In practice, it is useful to have a second convolution layer after the
        SubPixelUpscaling layer to speed up the learning process.
        However, if you are stacking multiple SubPixelUpscaling blocks, it may increase
        the number of parameters greatly, so the Convolution layer after SubPixelUpscaling
        layer can be removed.
    # Arguments
        scale_factor: Upscaling factor.
        data_format: Can be None, &#39;channels_first&#39; or &#39;channels_last&#39;.
    # Input shape
        4D tensor with shape:
        `(samples, k * (scale_factor * scale_factor) channels, rows, cols)` if data_format=&#39;channels_first&#39;
        or 4D tensor with shape:
        `(samples, rows, cols, k * (scale_factor * scale_factor) channels)` if data_format=&#39;channels_last&#39;.
    # Output shape
        4D tensor with shape:
        `(samples, k channels, rows * scale_factor, cols * scale_factor))` if data_format=&#39;channels_first&#39;
        or 4D tensor with shape:
        `(samples, rows * scale_factor, cols * scale_factor, k channels)` if data_format=&#39;channels_last&#39;.
    &#34;&#34;&#34;

    def __init__(self, scale_factor=2, data_format=None, **kwargs):
        super(SubPixelUpscaling, self).__init__(**kwargs)

        self.scale_factor = scale_factor
        self.data_format = normalize_data_format(data_format)

    def build(self, input_shape):
        pass

    def call(self, x, mask=None):
        y = depth_to_space(x, self.scale_factor, self.data_format)
        return y

    def compute_output_shape(self, input_shape):
        if self.data_format == &#34;channels_first&#34;:
            b, k, r, c = input_shape
            return (
                b,
                k // (self.scale_factor ** 2),
                r * self.scale_factor,
                c * self.scale_factor,
            )
        else:
            b, r, c, k = input_shape
            return (
                b,
                r * self.scale_factor,
                c * self.scale_factor,
                k // (self.scale_factor ** 2),
            )

    def get_config(self):
        config = {&#34;scale_factor&#34;: self.scale_factor, &#34;data_format&#34;: self.data_format}
        base_config = super(SubPixelUpscaling, self).get_config()
        return dict(list(base_config.items()) + list(config.items()))</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>tensorflow.python.keras.engine.base_layer.Layer</li>
<li>tensorflow.python.module.module.Module</li>
<li>tensorflow.python.training.tracking.tracking.AutoTrackable</li>
<li>tensorflow.python.training.tracking.base.Trackable</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="deepposekit.models.layers.convolutional.SubPixelUpscaling.build"><code class="name flex">
<span>def <span class="ident">build</span></span>(<span>self, input_shape)</span>
</code></dt>
<dd>
<section class="desc"><p>Creates the variables of the layer (optional, for subclass implementers).</p>
<p>This is a method that implementers of subclasses of <code>Layer</code> or <code>Model</code>
can override if they need a state-creation step in-between
layer instantiation and layer call.</p>
<p>This is typically used to create the weights of <code>Layer</code> subclasses.</p>
<h2 id="arguments">Arguments</h2>
<dl>
<dt><strong><code>input_shape</code></strong></dt>
<dd>Instance of <code>TensorShape</code>, or list of instances of
<code>TensorShape</code> if the layer expects a list of inputs
(one instance per input).</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def build(self, input_shape):
    pass</code></pre>
</details>
</dd>
<dt id="deepposekit.models.layers.convolutional.SubPixelUpscaling.call"><code class="name flex">
<span>def <span class="ident">call</span></span>(<span>self, x, mask=None)</span>
</code></dt>
<dd>
<section class="desc"><p>This is where the layer's logic lives.</p>
<h2 id="arguments">Arguments</h2>
<dl>
<dt><strong><code>inputs</code></strong></dt>
<dd>Input tensor, or list/tuple of input tensors.</dd>
<dt><strong><code>**kwargs</code></strong></dt>
<dd>Additional keyword arguments.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>A tensor or list/tuple of tensors.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def call(self, x, mask=None):
    y = depth_to_space(x, self.scale_factor, self.data_format)
    return y</code></pre>
</details>
</dd>
<dt id="deepposekit.models.layers.convolutional.SubPixelUpscaling.compute_output_shape"><code class="name flex">
<span>def <span class="ident">compute_output_shape</span></span>(<span>self, input_shape)</span>
</code></dt>
<dd>
<section class="desc"><p>Computes the output shape of the layer.</p>
<p>Assumes that the layer will be built
to match that input shape provided.</p>
<h2 id="arguments">Arguments</h2>
<dl>
<dt><strong><code>input_shape</code></strong></dt>
<dd>Shape tuple (tuple of integers)
or list of shape tuples (one per output tensor of the layer).
Shape tuples can include None for free dimensions,
instead of an integer.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>An input shape tuple.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def compute_output_shape(self, input_shape):
    if self.data_format == &#34;channels_first&#34;:
        b, k, r, c = input_shape
        return (
            b,
            k // (self.scale_factor ** 2),
            r * self.scale_factor,
            c * self.scale_factor,
        )
    else:
        b, r, c, k = input_shape
        return (
            b,
            r * self.scale_factor,
            c * self.scale_factor,
            k // (self.scale_factor ** 2),
        )</code></pre>
</details>
</dd>
<dt id="deepposekit.models.layers.convolutional.SubPixelUpscaling.get_config"><code class="name flex">
<span>def <span class="ident">get_config</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"><p>Returns the config of the layer.</p>
<p>A layer config is a Python dictionary (serializable)
containing the configuration of a layer.
The same layer can be reinstantiated later
(without its trained weights) from this configuration.</p>
<p>The config of a layer does not include connectivity
information, nor the layer class name. These are handled
by <code>Network</code> (one layer of abstraction above).</p>
<h2 id="returns">Returns</h2>
<p>Python dictionary.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_config(self):
    config = {&#34;scale_factor&#34;: self.scale_factor, &#34;data_format&#34;: self.data_format}
    base_config = super(SubPixelUpscaling, self).get_config()
    return dict(list(base_config.items()) + list(config.items()))</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="deepposekit.models.layers.convolutional.UpSampling2D"><code class="flex name class">
<span>class <span class="ident">UpSampling2D</span></span>
<span>(</span><span>size=(2, 2), data_format=None, interpolation='nearest', **kwargs)</span>
</code></dt>
<dd>
<section class="desc"><p>Upsampling layer for 2D inputs.</p>
<p>Repeats the rows and columns of the data
by <code>size[0]</code> and <code>size[1]</code> respectively.</p>
<h2 id="arguments">Arguments</h2>
<dl>
<dt><strong><code>size</code></strong></dt>
<dd>Int, or tuple of 2 integers.
The upsampling factors for rows and columns.</dd>
<dt><strong><code>data_format</code></strong></dt>
<dd>A string,
one of <code>channels_last</code> (default) or <code>channels_first</code>.
The ordering of the dimensions in the inputs.
<code>channels_last</code> corresponds to inputs with shape
<code>(batch, height, width, channels)</code> while <code>channels_first</code>
corresponds to inputs with shape
<code>(batch, channels, height, width)</code>.
It defaults to the <code>image_data_format</code> value found in your
Keras config file at <code>~/.keras/keras.json</code>.
If you never set it, then it will be "channels_last".</dd>
<dt><strong><code>interpolation</code></strong></dt>
<dd>A string, one of <code>nearest</code> or <code>bilinear</code>.</dd>
</dl>
<p>Input shape:
4D tensor with shape:
- If <code>data_format</code> is <code>"channels_last"</code>:
<code>(batch, rows, cols, channels)</code>
- If <code>data_format</code> is <code>"channels_first"</code>:
<code>(batch, channels, rows, cols)</code></p>
<p>Output shape:
4D tensor with shape:
- If <code>data_format</code> is <code>"channels_last"</code>:
<code>(batch, upsampled_rows, upsampled_cols, channels)</code>
- If <code>data_format</code> is <code>"channels_first"</code>:
<code>(batch, channels, upsampled_rows, upsampled_cols)</code></p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class UpSampling2D(Layer):
  &#34;&#34;&#34;Upsampling layer for 2D inputs.

  Repeats the rows and columns of the data
  by `size[0]` and `size[1]` respectively.

  Arguments:
    size: Int, or tuple of 2 integers.
      The upsampling factors for rows and columns.
    data_format: A string,
      one of `channels_last` (default) or `channels_first`.
      The ordering of the dimensions in the inputs.
      `channels_last` corresponds to inputs with shape
      `(batch, height, width, channels)` while `channels_first`
      corresponds to inputs with shape
      `(batch, channels, height, width)`.
      It defaults to the `image_data_format` value found in your
      Keras config file at `~/.keras/keras.json`.
      If you never set it, then it will be &#34;channels_last&#34;.
    interpolation: A string, one of `nearest` or `bilinear`.

  Input shape:
    4D tensor with shape:
    - If `data_format` is `&#34;channels_last&#34;`:
        `(batch, rows, cols, channels)`
    - If `data_format` is `&#34;channels_first&#34;`:
        `(batch, channels, rows, cols)`

  Output shape:
    4D tensor with shape:
    - If `data_format` is `&#34;channels_last&#34;`:
        `(batch, upsampled_rows, upsampled_cols, channels)`
    - If `data_format` is `&#34;channels_first&#34;`:
        `(batch, channels, upsampled_rows, upsampled_cols)`
  &#34;&#34;&#34;

  def __init__(self,
               size=(2, 2),
               data_format=None,
               interpolation=&#39;nearest&#39;,
               **kwargs):
    super(UpSampling2D, self).__init__(**kwargs)
    self.data_format = conv_utils.normalize_data_format(data_format)
    self.size = conv_utils.normalize_tuple(size, 2, &#39;size&#39;)
    if interpolation not in {&#39;nearest&#39;, &#39;bilinear&#39;}:
      raise ValueError(&#39;`interpolation` argument should be one of `&#34;nearest&#34;` &#39;
                       &#39;or `&#34;bilinear&#34;`.&#39;)
    self.interpolation = interpolation
    self.input_spec = InputSpec(ndim=4)

  def compute_output_shape(self, input_shape):
    input_shape = tensor_shape.TensorShape(input_shape).as_list()
    if self.data_format == &#39;channels_first&#39;:
      height = self.size[0] * input_shape[
          2] if input_shape[2] is not None else None
      width = self.size[1] * input_shape[
          3] if input_shape[3] is not None else None
      return tensor_shape.TensorShape(
          [input_shape[0], input_shape[1], height, width])
    else:
      height = self.size[0] * input_shape[
          1] if input_shape[1] is not None else None
      width = self.size[1] * input_shape[
          2] if input_shape[2] is not None else None
      return tensor_shape.TensorShape(
          [input_shape[0], height, width, input_shape[3]])

  def call(self, inputs):
    return backend.resize_images(
        inputs, self.size[0], self.size[1], self.data_format,
        interpolation=self.interpolation)

  def get_config(self):
    config = {&#39;size&#39;: self.size, &#39;data_format&#39;: self.data_format}
    base_config = super(UpSampling2D, self).get_config()
    return dict(list(base_config.items()) + list(config.items()))</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>tensorflow.python.keras.engine.base_layer.Layer</li>
<li>tensorflow.python.module.module.Module</li>
<li>tensorflow.python.training.tracking.tracking.AutoTrackable</li>
<li>tensorflow.python.training.tracking.base.Trackable</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="deepposekit.models.layers.convolutional.UpSampling2D.call"><code class="name flex">
<span>def <span class="ident">call</span></span>(<span>self, inputs)</span>
</code></dt>
<dd>
<section class="desc"><p>This is where the layer's logic lives.</p>
<h2 id="arguments">Arguments</h2>
<dl>
<dt><strong><code>inputs</code></strong></dt>
<dd>Input tensor, or list/tuple of input tensors.</dd>
<dt><strong><code>**kwargs</code></strong></dt>
<dd>Additional keyword arguments.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>A tensor or list/tuple of tensors.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def call(self, inputs):
  return backend.resize_images(
      inputs, self.size[0], self.size[1], self.data_format,
      interpolation=self.interpolation)</code></pre>
</details>
</dd>
<dt id="deepposekit.models.layers.convolutional.UpSampling2D.compute_output_shape"><code class="name flex">
<span>def <span class="ident">compute_output_shape</span></span>(<span>self, input_shape)</span>
</code></dt>
<dd>
<section class="desc"><p>Computes the output shape of the layer.</p>
<p>Assumes that the layer will be built
to match that input shape provided.</p>
<h2 id="arguments">Arguments</h2>
<dl>
<dt><strong><code>input_shape</code></strong></dt>
<dd>Shape tuple (tuple of integers)
or list of shape tuples (one per output tensor of the layer).
Shape tuples can include None for free dimensions,
instead of an integer.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>An input shape tuple.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def compute_output_shape(self, input_shape):
  input_shape = tensor_shape.TensorShape(input_shape).as_list()
  if self.data_format == &#39;channels_first&#39;:
    height = self.size[0] * input_shape[
        2] if input_shape[2] is not None else None
    width = self.size[1] * input_shape[
        3] if input_shape[3] is not None else None
    return tensor_shape.TensorShape(
        [input_shape[0], input_shape[1], height, width])
  else:
    height = self.size[0] * input_shape[
        1] if input_shape[1] is not None else None
    width = self.size[1] * input_shape[
        2] if input_shape[2] is not None else None
    return tensor_shape.TensorShape(
        [input_shape[0], height, width, input_shape[3]])</code></pre>
</details>
</dd>
<dt id="deepposekit.models.layers.convolutional.UpSampling2D.get_config"><code class="name flex">
<span>def <span class="ident">get_config</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"><p>Returns the config of the layer.</p>
<p>A layer config is a Python dictionary (serializable)
containing the configuration of a layer.
The same layer can be reinstantiated later
(without its trained weights) from this configuration.</p>
<p>The config of a layer does not include connectivity
information, nor the layer class name. These are handled
by <code>Network</code> (one layer of abstraction above).</p>
<h2 id="returns">Returns</h2>
<p>Python dictionary.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_config(self):
  config = {&#39;size&#39;: self.size, &#39;data_format&#39;: self.data_format}
  base_config = super(UpSampling2D, self).get_config()
  return dict(list(base_config.items()) + list(config.items()))</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="deepposekit.models.layers" href="index.html">deepposekit.models.layers</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="deepposekit.models.layers.convolutional.Maxima2D" href="#deepposekit.models.layers.convolutional.Maxima2D">Maxima2D</a></code></h4>
<ul class="">
<li><code><a title="deepposekit.models.layers.convolutional.Maxima2D.call" href="#deepposekit.models.layers.convolutional.Maxima2D.call">call</a></code></li>
<li><code><a title="deepposekit.models.layers.convolutional.Maxima2D.compute_output_shape" href="#deepposekit.models.layers.convolutional.Maxima2D.compute_output_shape">compute_output_shape</a></code></li>
<li><code><a title="deepposekit.models.layers.convolutional.Maxima2D.get_config" href="#deepposekit.models.layers.convolutional.Maxima2D.get_config">get_config</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="deepposekit.models.layers.convolutional.SubPixelDownscaling" href="#deepposekit.models.layers.convolutional.SubPixelDownscaling">SubPixelDownscaling</a></code></h4>
<ul class="">
<li><code><a title="deepposekit.models.layers.convolutional.SubPixelDownscaling.build" href="#deepposekit.models.layers.convolutional.SubPixelDownscaling.build">build</a></code></li>
<li><code><a title="deepposekit.models.layers.convolutional.SubPixelDownscaling.call" href="#deepposekit.models.layers.convolutional.SubPixelDownscaling.call">call</a></code></li>
<li><code><a title="deepposekit.models.layers.convolutional.SubPixelDownscaling.compute_output_shape" href="#deepposekit.models.layers.convolutional.SubPixelDownscaling.compute_output_shape">compute_output_shape</a></code></li>
<li><code><a title="deepposekit.models.layers.convolutional.SubPixelDownscaling.get_config" href="#deepposekit.models.layers.convolutional.SubPixelDownscaling.get_config">get_config</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="deepposekit.models.layers.convolutional.SubPixelUpscaling" href="#deepposekit.models.layers.convolutional.SubPixelUpscaling">SubPixelUpscaling</a></code></h4>
<ul class="">
<li><code><a title="deepposekit.models.layers.convolutional.SubPixelUpscaling.build" href="#deepposekit.models.layers.convolutional.SubPixelUpscaling.build">build</a></code></li>
<li><code><a title="deepposekit.models.layers.convolutional.SubPixelUpscaling.call" href="#deepposekit.models.layers.convolutional.SubPixelUpscaling.call">call</a></code></li>
<li><code><a title="deepposekit.models.layers.convolutional.SubPixelUpscaling.compute_output_shape" href="#deepposekit.models.layers.convolutional.SubPixelUpscaling.compute_output_shape">compute_output_shape</a></code></li>
<li><code><a title="deepposekit.models.layers.convolutional.SubPixelUpscaling.get_config" href="#deepposekit.models.layers.convolutional.SubPixelUpscaling.get_config">get_config</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="deepposekit.models.layers.convolutional.UpSampling2D" href="#deepposekit.models.layers.convolutional.UpSampling2D">UpSampling2D</a></code></h4>
<ul class="">
<li><code><a title="deepposekit.models.layers.convolutional.UpSampling2D.call" href="#deepposekit.models.layers.convolutional.UpSampling2D.call">call</a></code></li>
<li><code><a title="deepposekit.models.layers.convolutional.UpSampling2D.compute_output_shape" href="#deepposekit.models.layers.convolutional.UpSampling2D.compute_output_shape">compute_output_shape</a></code></li>
<li><code><a title="deepposekit.models.layers.convolutional.UpSampling2D.get_config" href="#deepposekit.models.layers.convolutional.UpSampling2D.get_config">get_config</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.7.0</a>.</p>
</footer>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad()</script>
</body>
</html>